{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#import os, sys; sys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n",
    "import os\n",
    "from pattern.web import Twitter\n",
    "from pattern.en import Sentence, parse\n",
    "from pattern.search import search\n",
    "from pattern.vector import Document, Model, KNN\n",
    "\n",
    "# Classification is a supervised machine learning method,\n",
    "# where labeled documents are used as training material\n",
    "# to learn how to label unlabeled documents.\n",
    "\n",
    "# This example trains a simple classifier with Twitter messages.\n",
    "# The idea is that, if you have a number of texts with a \"type\"\n",
    "# (mail/spam, positive/negative, language, author's age, ...),\n",
    "# you can predict the type of other \"unknown\" texts.\n",
    "# The k-Nearest Neighbor algorithm classifies texts according\n",
    "# to the k documents that are most similar (cosine similarity) to the given input document.\n",
    "\n",
    "m = Model()\n",
    "#t = Twitter()\n",
    "file_object = open('raw_text_full', 'r', encoding=\"utf-8\")\n",
    "line_count = 0\n",
    "for line in file_object:\n",
    "    if(len(line)>0):\n",
    "    #print(line)\n",
    "# First, we mine a model of a 1000 tweets.\n",
    "# We'll use hashtags as type.\n",
    "#for page in range(1, 10):\n",
    "#    for tweet in t.search('#win OR #fail', start=page, count=100, cached=True):\n",
    "        # If the tweet contains #win hashtag, we'll set its type to 'WIN':\n",
    "        #print(line)\n",
    "        s = line.lower()               # tweet in lowercase\n",
    "        if('help' in s or 'urgent' in s or 'emergency' in s ):\n",
    "            p = 'HELP!'\n",
    "        else:\n",
    "            p = 'FINE'  # document labels      \n",
    "        #s = Sentence(parse(s))               # parse tree with part-of-speech tags\n",
    "        #s = search('JJ', s)                  # adjectives in the tweet\n",
    "        #s = [match[0].string for match in s] # adjectives as a list of strings\n",
    "        #s = \" \".join(s)                      # adjectives as string\n",
    "        #if len(s) > 0:\n",
    "        m.append(Document(s, type=p, stemmer=None))\n",
    "file_object.close()\n",
    "# Train k-Nearest Neighbor on the model.\n",
    "# Note that this is a only simple example: to build a robust classifier\n",
    "# you would need a lot more training data (e.g., tens of thousands of tweets).\n",
    "# The more training data, the more statistically reliable the classifier becomes.\n",
    "# The only way to really know if you're classifier is working correctly\n",
    "# is to test it with testing data, see the documentation for Classifier.test().\n",
    "classifier = KNN(baseline=None) # By default, baseline=MAJORITY\n",
    "for document in m:              # (classify unknown documents with the most frequent type).\n",
    "    classifier.train(document)\n",
    "\n",
    "# These are the adjectives the classifier has learned:\n",
    "#print(sorted(classifier.features))\n",
    "print()\n",
    "#m.save('saved_model', update=False)\n",
    "classifier.save('saved_classifier_full')\n",
    "# We can now ask it to classify documents containing these words.\n",
    "# Note that you may get different results than the ones below,\n",
    "# since you will be mining other (more recent) tweets.\n",
    "# Again, a robust classifier needs lots and lots of training data.\n",
    "# If None is returned, the word was not recognized,\n",
    "# and the classifier returned the default value (see above).\n",
    "\n",
    "#print(classifier.classify('arc chance')) # yields 'WIN'\n",
    "#print(classifier.classify('stupid autocorrect'))  # yields 'FAIL'\n",
    "print('DONE')\n",
    "# \"What can I do with it?\"\n",
    "# In the scientific community, classifiers have been used to predict:\n",
    "# - the opinion (positive/negative) in product reviews on blogs,\n",
    "# - the age of users posting on social networks,\n",
    "# - the author of medieval poems,\n",
    "# - spam in  e-mail messages,\n",
    "# - lies & deception in text,\n",
    "# - doubt & uncertainty in text,\n",
    "# and to:\n",
    "# - improve search engine query results (e.g., where \"jeans\" queries also yield \"denim\" results),\n",
    "# - win at Jeopardy!,\n",
    "# - win at rock-paper-scissors,\n",
    "# and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
